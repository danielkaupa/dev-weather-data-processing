{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1e6c5d",
   "metadata": {},
   "source": [
    "## Rough Implementation of Data Cleaning and Joining Module - Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86422c3e",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0907404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Data Manipulation & Analysis\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Geospatial Data Handling\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import xarray as xr\n",
    "from xarray.coding.times import CFTimedeltaCoder\n",
    "import cfgrib\n",
    "# import xesmf as xe\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Notebook/Display Tools\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.features import RegularPolygonMarker\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# System / Miscellaneous\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import logging\n",
    "import re\n",
    "import concurrent.futures\n",
    "\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from typing import List, Optional, Dict, Set, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from eccodes import (\n",
    "    codes_grib_new_from_file,\n",
    "    codes_get,\n",
    "    codes_release,\n",
    "    CodesInternalError,\n",
    ")\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17fc88",
   "metadata": {},
   "source": [
    "### Initial Setup (directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d483175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Current Working Directory and contents:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing\n",
      "Subdirectories: ['.git']\n",
      "----------------------------------------\n",
      "-> File: README.md\n",
      "-> File: data_cleaning_development.ipynb\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git\n",
      "Subdirectories: ['objects', 'info', 'logs', 'hooks', 'refs']\n",
      "----------------------------------------\n",
      "-> File: HEAD\n",
      "-> File: config\n",
      "-> File: description\n",
      "-> File: index\n",
      "-> File: packed-refs\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/objects\n",
      "Subdirectories: ['pack', 'info']\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/objects/pack\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: pack-d71363e60bf6a5848a395e516108500aa2534b55.idx\n",
      "-> File: pack-d71363e60bf6a5848a395e516108500aa2534b55.pack\n",
      "-> File: pack-d71363e60bf6a5848a395e516108500aa2534b55.rev\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/objects/info\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/info\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: exclude\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/logs\n",
      "Subdirectories: ['refs']\n",
      "----------------------------------------\n",
      "-> File: HEAD\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/logs/refs\n",
      "Subdirectories: ['heads', 'remotes']\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/logs/refs/heads\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: main\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/logs/refs/remotes\n",
      "Subdirectories: ['origin']\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/logs/refs/remotes/origin\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: HEAD\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/hooks\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: applypatch-msg.sample\n",
      "-> File: commit-msg.sample\n",
      "-> File: fsmonitor-watchman.sample\n",
      "-> File: post-update.sample\n",
      "-> File: pre-applypatch.sample\n",
      "-> File: pre-commit.sample\n",
      "-> File: pre-merge-commit.sample\n",
      "-> File: pre-push.sample\n",
      "-> File: pre-rebase.sample\n",
      "-> File: pre-receive.sample\n",
      "-> File: prepare-commit-msg.sample\n",
      "-> File: push-to-checkout.sample\n",
      "-> File: sendemail-validate.sample\n",
      "-> File: update.sample\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/refs\n",
      "Subdirectories: ['heads', 'tags', 'remotes']\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/refs/heads\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: main\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/refs/tags\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/refs/remotes\n",
      "Subdirectories: ['origin']\n",
      "----------------------------------------\n",
      "\n",
      "Directory: /Users/Daniel/Desktop/dev-weather-data-processing/.git/refs/remotes/origin\n",
      "Subdirectories: []\n",
      "----------------------------------------\n",
      "-> File: HEAD\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"-\"*120)\n",
    "print(\"Current Working Directory and contents:\\n\"+\"-\"*120)\n",
    "for root, dirs, files in os.walk(cwd):\n",
    "    print(f\"\\nDirectory: {root}\")\n",
    "    print(f\"Subdirectories: {dirs}\\n\"+ \"-\"*40)\n",
    "    for file in sorted(files):\n",
    "        print(f\"-> File: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a12697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Daniel/Desktop\n",
      "/Users/Daniel/Desktop/data\n",
      "/Users/Daniel/Desktop/data/raw\n",
      "contents of raw data directory:\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2019_08.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2024_03.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2020_01.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2019_12.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2018_01.grib.5b7b6.idx\n",
      "/Users/Daniel/Desktop/data/raw/.DS_Store\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2025_04.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2018_02.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2024_04.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2025_06.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2018_01.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2023_02.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2024_01.grib\n",
      "/Users/Daniel/Desktop/data/raw/era5-world_N37W68S6E98_d514a3a3c256_2023_12.grib\n"
     ]
    }
   ],
   "source": [
    "# DIRECTORIES AND PATHS\n",
    "root_directory = Path.cwd().parent\n",
    "print(root_directory)\n",
    "\n",
    "data_directory = root_directory / \"data\"\n",
    "print(data_directory)\n",
    "\n",
    "data_raw_directory = data_directory / \"raw\"\n",
    "print(data_raw_directory)\n",
    "\n",
    "print(\"contents of raw data directory:\")\n",
    "for item in data_raw_directory.iterdir():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195496d1",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4319074",
   "metadata": {},
   "source": [
    "#### Part 1 - Identify and Select Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ea92e",
   "metadata": {},
   "source": [
    "##### Functions - Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "873244e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(fname: str) -> Optional[tuple[str, int, int, str]]:\n",
    "    \"\"\"\n",
    "    Parse ERA5-style filenames of the form:\n",
    "        {prefix}_{YYYY}_{MM}[.ext]\n",
    "\n",
    "    Assumes:\n",
    "    - The filename contains exactly one dot before the extension.\n",
    "    - The last two underscore-separated tokens before the dot are YYYY and MM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        Filename to parse.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, int, int, str] | None\n",
    "        prefix (str), year (int), month (int), extension (str) if parsing is successful, else None.\n",
    "    \"\"\"\n",
    "    # Split into basename and extension\n",
    "    root, extension = fname.split(\".\", 1)\n",
    "\n",
    "    # Split root into prefix parts\n",
    "    parts = root.split(\"_\")\n",
    "\n",
    "    # Need at least: prefix_part(s) + YYYY + MM\n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "\n",
    "    year_str = parts[-2]\n",
    "    month_str = parts[-1]\n",
    "\n",
    "    # Validate numeric year/month\n",
    "    if not (year_str.isdigit() and month_str.isdigit()):\n",
    "        return None\n",
    "\n",
    "    prefix = \"_\".join(parts[:-2])\n",
    "    year = int(year_str)\n",
    "    month = int(month_str)\n",
    "\n",
    "    return prefix, year, month, extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec8f3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory(raw_dir: Path) -> Dict[str, Dict[int, List[Path]]]:\n",
    "    \"\"\"\n",
    "    Scan directory (assuming raw data) and build structured mapping using the format\n",
    "    assumed by parse_filename() function (key, year, month, extension).\n",
    "\n",
    "    Key format for mapping: tuple(prefix, extension)\n",
    "\n",
    "    Sample Output structure:\n",
    "    {\n",
    "        (\"prefix1\", \"grib\"): {\n",
    "             2018: [Path(...), Path(...)],\n",
    "             2019: [...],\n",
    "        },\n",
    "        (\"prefix2\", \"nc\"): { ... }\n",
    "    }\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_dir: Path\n",
    "        Path to the raw data directory to be scanned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[int, list[Path]]]\n",
    "        Nested dictionary mapping dataset keys to years and lists of file paths.\n",
    "        [str: dataset key, int: year, list[Path]: list of file paths]\n",
    "    \"\"\"\n",
    "    mapping: Dict[str, Dict[int, List[Path]]] = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for entry in raw_dir.iterdir():\n",
    "        if entry.is_file() and not entry.name.startswith(\".\"):\n",
    "            parsed = parse_filename(entry.name)\n",
    "            if parsed is None:\n",
    "                continue\n",
    "\n",
    "            prefix, year, month, extension = parsed\n",
    "            dataset_key = f\"{prefix} ({extension})\"\n",
    "            mapping[dataset_key][year].append(entry)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb5e2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_summary(\n",
    "        mapping: dict,\n",
    "        directory: str\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Print the dataset prefixes, years, and months in the raw directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping: dict\n",
    "        Nested dictionary mapping dataset keys to years and lists of file paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Prints the summary to console.\n",
    "    \"\"\"\n",
    "    print(f\"\\nDetected the following datasets in [{directory}]:\\n\")\n",
    "\n",
    "    for dataset_key, years in mapping.items():\n",
    "        print(\"-\"*40 + f\"\\n{dataset_key}\\n\" + \"-\"*40)\n",
    "        print(f\"\\tYears found: {', '.join(str(y) for y in sorted(years))}\")\n",
    "\n",
    "        for year in sorted(years):\n",
    "            months = []\n",
    "            for path in years[year]:\n",
    "                _, _, month, _ = parse_filename(path.name)\n",
    "                months.append(month)\n",
    "\n",
    "            mon_str = \", \".join(f\"{m:02d}\" for m in sorted(months))\n",
    "            print(f\"\\t\\t{year}: {mon_str}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8a1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_and_optionally_print(\n",
    "        raw_dir: Path,\n",
    "        print_summary: bool = True\n",
    "        ) -> dict[str, dict[int, list[Path]]]:\n",
    "    \"\"\"\n",
    "    Combined function to scan the directory and optionally print the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_dir : Path\n",
    "        Directory containing raw ERA5 files\n",
    "    print_summary : bool\n",
    "        If True, prints the summary of datasets found\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict[int, list[Path]]]\n",
    "        Mapping of dataset_key → years → list of files\n",
    "    \"\"\"\n",
    "    mapping = scan_directory(raw_dir)\n",
    "\n",
    "    if print_summary:\n",
    "        print_dataset_summary(mapping, directory=raw_dir)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c5ea6",
   "metadata": {},
   "source": [
    "##### Functions - Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a6b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_user_for_directory(default_dir: str = \"data/raw\") -> Path:\n",
    "    \"\"\"\n",
    "    Prompt the user to specify a directory relative to the project root.\n",
    "    Validates that the directory exists. Falls back to default if desired.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    default_dir : str\n",
    "        The default directory to use if the user just presses ENTER.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        A valid directory path pointing to the selected folder.\n",
    "    \"\"\"\n",
    "    print(\"\\nPlease enter the relative directory you want to scan.\")\n",
    "    print(f\"Press ENTER to use the default: [{default_dir}]\")\n",
    "    print(\"Example inputs: data/raw, data/new_data, datasets/era5, etc.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Directory: \").strip()\n",
    "\n",
    "        # User pressed ENTER → use default\n",
    "        if user_input == \"\":\n",
    "            chosen = Path(default_dir)\n",
    "        else:\n",
    "            chosen = Path(user_input)\n",
    "\n",
    "        if chosen.exists() and chosen.is_dir():\n",
    "            print(f\"\\nUsing directory: {chosen.resolve()}\\n\")\n",
    "            return chosen\n",
    "        else:\n",
    "            print(f\"[ERROR] Directory '{chosen}' does not exist. Please try again.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dc9a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_user_for_dataset_key(mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the user to select one dataset key from the provided mapping.\n",
    "\n",
    "    This function displays all available dataset identifiers (keys) generated\n",
    "    by ``scan_directory`` or ``scan_and_optionally_print``. The user may select\n",
    "    a dataset either by typing the full dataset key or by entering the\n",
    "    corresponding number shown in the list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping : dict\n",
    "        A nested dictionary of the form::\n",
    "\n",
    "            {\n",
    "                \"prefix_x (ext)\": {\n",
    "                    2018: [Path(...), Path(...), ...],\n",
    "                    2019: [...],\n",
    "                    ...\n",
    "                },\n",
    "                \"prefix_y (ext)\": { ... },\n",
    "                ...\n",
    "            }\n",
    "\n",
    "        Keys correspond to dataset identifiers of the form\n",
    "        ``\"{prefix} ({extension})\"``.\n",
    "\n",
    "        Values are dictionaries mapping integer years to lists of ``Path`` objects\n",
    "        referencing the monthly dataset files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The dataset key chosen by the user. This string will match exactly one\n",
    "        of the keys present in ``mapping``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Input is validated to ensure that only valid selections are accepted.\n",
    "    - The function loops until a correct response is provided.\n",
    "    - This function is intended for CLI or notebook interactive workflows.\n",
    "    - The returned key can be used to index into ``mapping`` to access\n",
    "      the associated file paths.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> mapping = {\n",
    "    ...     \"era5-delhi (grib)\": {2020: [Path(\"file1\")], 2021: [Path(\"file2\")]},\n",
    "    ...     \"era5-mumbai (grib)\": {2019: [Path(\"file3\")]}\n",
    "    ... }\n",
    "    >>> key = prompt_user_for_dataset_key(mapping)\n",
    "    Available dataset prefixes:\n",
    "      1. era5-delhi (grib)\n",
    "      2. era5-mumbai (grib)\n",
    "    Please enter the FULL dataset key above, or enter its number: 1\n",
    "    You selected: era5-delhi (grib)\n",
    "    >>> key\n",
    "    'era5-delhi (grib)'\n",
    "    \"\"\"\n",
    "    dataset_keys = sorted(mapping.keys())\n",
    "\n",
    "    print(\"\\nAvailable dataset prefixes:\")\n",
    "    for i, key in enumerate(dataset_keys, start=1):\n",
    "        print(f\"  {i}. {key}\")\n",
    "\n",
    "    while True:\n",
    "        selection = input(\n",
    "            \"\\nPlease enter the FULL dataset key above, or enter its number: \"\n",
    "        ).strip()\n",
    "\n",
    "        # Case 1: user entered a number\n",
    "        if selection.isdigit():\n",
    "            idx = int(selection) - 1\n",
    "            if 0 <= idx < len(dataset_keys):\n",
    "                chosen = dataset_keys[idx]\n",
    "                print(f\"\\nYou selected: {chosen}\\n\")\n",
    "                return chosen\n",
    "            else:\n",
    "                print(\"[ERROR] Invalid number. Try again.\")\n",
    "\n",
    "        # Case 2: user entered full string dataset key\n",
    "        elif selection in dataset_keys:\n",
    "            print(f\"\\nYou selected: {selection}\\n\")\n",
    "            return selection\n",
    "\n",
    "        # Case 3: invalid input\n",
    "        else:\n",
    "            print(\"[ERROR] Invalid input. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad398511",
   "metadata": {},
   "source": [
    "##### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac8c4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the relative directory you want to scan.\n",
      "Press ENTER to use the default: [data/raw]\n",
      "Example inputs: data/raw, data/new_data, datasets/era5, etc.\n",
      "\n",
      "[ERROR] Directory 'data/raw' does not exist. Please try again.\n",
      "\n",
      "\n",
      "Using directory: /Users/Daniel/Desktop/data/raw\n",
      "\n",
      "\n",
      "Detected the following datasets in [../data/raw]:\n",
      "\n",
      "----------------------------------------\n",
      "era5-world_N37W68S6E98_d514a3a3c256 (grib)\n",
      "----------------------------------------\n",
      "\tYears found: 2018, 2019, 2020, 2023, 2024, 2025\n",
      "\t\t2018: 01, 02\n",
      "\t\t2019: 08, 12\n",
      "\t\t2020: 01\n",
      "\t\t2023: 02, 12\n",
      "\t\t2024: 01, 03, 04\n",
      "\t\t2025: 04, 06\n",
      "\n",
      "----------------------------------------\n",
      "era5-world_N37W68S6E98_d514a3a3c256 (grib.5b7b6.idx)\n",
      "----------------------------------------\n",
      "\tYears found: 2018\n",
      "\t\t2018: 01\n",
      "\n",
      "\n",
      "Available dataset prefixes:\n",
      "  1. era5-world_N37W68S6E98_d514a3a3c256 (grib)\n",
      "  2. era5-world_N37W68S6E98_d514a3a3c256 (grib.5b7b6.idx)\n",
      "\n",
      "You selected: era5-world_N37W68S6E98_d514a3a3c256 (grib)\n",
      "\n",
      "You will now process dataset: era5-world_N37W68S6E98_d514a3a3c256 (grib)\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION WORKFLOW ---\n",
    "\n",
    "# 1. Ask the user which directory to scan\n",
    "RAW_DIR = prompt_user_for_directory(default_dir=\"data/raw\")\n",
    "\n",
    "# 2. Scan the directory and optionally print a summary\n",
    "mapping = scan_and_optionally_print(RAW_DIR, print_summary=True)\n",
    "\n",
    "# 3. Prompt user to choose which dataset key to process\n",
    "selected_key = prompt_user_for_dataset_key(mapping)\n",
    "\n",
    "print(f\"You will now process dataset: {selected_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfac7a2",
   "metadata": {},
   "source": [
    "#### Part 2 - Scanning File Contents & Processing Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c1079",
   "metadata": {},
   "source": [
    "##### Functions - Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "200398c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_file_for_dataset(files_by_year: Dict[int, List[Path]]) -> Path:\n",
    "    \"\"\"\n",
    "    Return the first available file (by sorted year, then path) for a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files_by_year : dict[int, list[pathlib.Path]]\n",
    "        Mapping of year → list of file paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Path to the first file.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If no files are available.\n",
    "    \"\"\"\n",
    "    if not files_by_year:\n",
    "        raise ValueError(\"No files available for the dataset.\")\n",
    "\n",
    "    first_year = min(files_by_year.keys())\n",
    "    if not files_by_year[first_year]:\n",
    "        raise ValueError(f\"No files found for year {first_year}.\")\n",
    "\n",
    "    return sorted(files_by_year[first_year])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17b3c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/era5-world_N37W68S6E98_d514a3a3c256_2018_01.grib\n"
     ]
    }
   ],
   "source": [
    "path = get_first_file_for_dataset(mapping[selected_key])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e7d2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_variables_in_file(path: Path) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scan a GRIB file with ecCodes and return a list of dictionaries:\n",
    "    [\n",
    "        { \"paramId\": 167, \"shortName\": \"2t\" },\n",
    "        { \"paramId\": 228, \"shortName\": \"tp\" },\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    - Uses low-level eccodes API.\n",
    "    - Works for ANY ERA5 world file.\n",
    "    - Robust to mixed editions, hours, time steps.\n",
    "    \"\"\"\n",
    "    variables = {}\n",
    "    # Use dict keyed by paramId to dedupe\n",
    "\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    gid = codes_grib_new_from_file(f)\n",
    "                except CodesInternalError:\n",
    "                    break\n",
    "                if gid is None:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    short_name = codes_get(gid, \"shortName\")\n",
    "                    param_id = codes_get(gid, \"paramId\")\n",
    "\n",
    "                    # decode if bytes\n",
    "                    if isinstance(short_name, bytes):\n",
    "                        short_name = short_name.decode(\"utf-8\")\n",
    "\n",
    "                    variables[param_id] = {\n",
    "                        \"paramId\": param_id,\n",
    "                        \"shortName\": short_name,\n",
    "                    }\n",
    "\n",
    "                finally:\n",
    "                    codes_release(gid)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while scanning {path}: {e}\")\n",
    "\n",
    "    return list(variables.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0ef57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup headless Chrome driver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def get_parameter_details_selenium(param_id: int, driver) -> Dict:\n",
    "    \"\"\"\n",
    "    Get parameter details using Selenium to render JavaScript\n",
    "    \"\"\"\n",
    "    url = f\"https://codes.ecmwf.int/grib/param-db/{param_id}/\"\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the content to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//td[contains(., 'Name')]\"))\n",
    "        )\n",
    "\n",
    "        details = {}\n",
    "\n",
    "        # Extract Name\n",
    "        try:\n",
    "            name_cell = driver.find_element(By.XPATH, \"//td[p[contains(., 'Name')]]/following-sibling::td\")\n",
    "            details['name'] = name_cell.find_element(By.TAG_NAME, \"p\").text\n",
    "        except:\n",
    "            details['name'] = ''\n",
    "\n",
    "        # Extract Unit\n",
    "        try:\n",
    "            unit_cell = driver.find_element(By.XPATH, \"//td[p[contains(., 'Unit')]]/following-sibling::td\")\n",
    "            details['unit'] = unit_cell.find_element(By.TAG_NAME, \"p\").text\n",
    "        except:\n",
    "            details['unit'] = ''\n",
    "\n",
    "        # Extract Description\n",
    "        try:\n",
    "            desc_cell = driver.find_element(By.XPATH, \"//td[p[contains(., 'Description')]]/following-sibling::td\")\n",
    "            details['description'] = desc_cell.find_element(By.TAG_NAME, \"p\").text\n",
    "        except:\n",
    "            details['description'] = ''\n",
    "\n",
    "        return details\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching param {param_id}: {e}\")\n",
    "        return {'name': '', 'unit': '', 'description': ''}\n",
    "\n",
    "def enrich_parameters_selenium(parameter_list: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Enrich parameter list using Selenium\n",
    "    \"\"\"\n",
    "    driver = setup_driver()\n",
    "    enriched = []\n",
    "\n",
    "    try:\n",
    "        for i, param in enumerate(parameter_list):\n",
    "            param_id = param['paramId']\n",
    "            short_name = param['shortName']\n",
    "\n",
    "            print(f\"Processing {short_name} (ID: {param_id}) - {i+1}/{len(parameter_list)}\")\n",
    "\n",
    "            details = get_parameter_details_selenium(param_id, driver)\n",
    "\n",
    "            # Create enriched parameter\n",
    "            enriched_param = param.copy()\n",
    "            enriched_param.update({\n",
    "                'full_name': details.get('name', ''),\n",
    "                'units': details.get('unit', ''),\n",
    "                'description': details.get('description', '')\n",
    "            })\n",
    "\n",
    "            enriched.append(enriched_param)\n",
    "\n",
    "            # Be polite to the server\n",
    "            time.sleep(1)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d79f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "def enrich_variable_list_selenium(vars_list: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Enrich a list of GRIB variable dicts using Selenium to extract metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vars_list : List[Dict]\n",
    "        List of dicts with 'paramId' and 'shortName'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict]\n",
    "        Enriched variable list with full name, units, and description.\n",
    "    \"\"\"\n",
    "    driver = setup_driver()\n",
    "    enriched = []\n",
    "\n",
    "    try:\n",
    "        for i, item in enumerate(vars_list):\n",
    "            param_id = item[\"paramId\"]\n",
    "            short_name = item[\"shortName\"]\n",
    "            print(f\"Processing {short_name} (ID: {param_id}) - {i+1}/{len(vars_list)}\")\n",
    "\n",
    "            try:\n",
    "                details = get_parameter_details_selenium(param_id, driver)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to fetch details for paramId {param_id}: {e}\")\n",
    "                details = {'name': '', 'unit': '', 'description': ''}\n",
    "\n",
    "            enriched_item = {\n",
    "                \"paramId\": param_id,\n",
    "                \"shortName\": short_name,\n",
    "                \"fullName\": details.get(\"name\", ''),\n",
    "                \"units\": details.get(\"unit\", ''),\n",
    "                \"description\": details.get(\"description\", ''),\n",
    "                \"url\": f\"https://codes.ecmwf.int/grib/param-db/{param_id}/\"\n",
    "            }\n",
    "            enriched.append(enriched_item)\n",
    "\n",
    "            # Respectful delay to avoid hammering ECMWF's servers\n",
    "            time.sleep(1)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc9135fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'paramId': 167, 'shortName': '2t'}, {'paramId': 228, 'shortName': 'tp'}, {'paramId': 165, 'shortName': '10u'}, {'paramId': 166, 'shortName': '10v'}, {'paramId': 228246, 'shortName': '100u'}, {'paramId': 228247, 'shortName': '100v'}, {'paramId': 228022, 'shortName': 'cdir'}, {'paramId': 57, 'shortName': 'uvb'}, {'paramId': 176, 'shortName': 'ssr'}, {'paramId': 210, 'shortName': 'ssrc'}, {'paramId': 177, 'shortName': 'str'}, {'paramId': 211, 'shortName': 'strc'}, {'paramId': 228129, 'shortName': 'ssrdc'}, {'paramId': 169, 'shortName': 'ssrd'}, {'paramId': 228130, 'shortName': 'strdc'}, {'paramId': 175, 'shortName': 'strd'}, {'paramId': 178, 'shortName': 'tsr'}, {'paramId': 208, 'shortName': 'tsrc'}, {'paramId': 179, 'shortName': 'ttr'}, {'paramId': 209, 'shortName': 'ttrc'}, {'paramId': 228021, 'shortName': 'fdir'}, {'paramId': 188, 'shortName': 'hcc'}, {'paramId': 186, 'shortName': 'lcc'}, {'paramId': 187, 'shortName': 'mcc'}, {'paramId': 164, 'shortName': 'tcc'}, {'paramId': 28, 'shortName': 'cvh'}, {'paramId': 67, 'shortName': 'lai_hv'}, {'paramId': 66, 'shortName': 'lai_lv'}, {'paramId': 27, 'shortName': 'cvl'}, {'paramId': 260121, 'shortName': 'kx'}]\n"
     ]
    }
   ],
   "source": [
    "vars = scan_variables_in_file(path)\n",
    "print(vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02bd4952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2t (ID: 167) - 1/30\n",
      "Processing tp (ID: 228) - 2/30\n",
      "Processing 10u (ID: 165) - 3/30\n",
      "Processing 10v (ID: 166) - 4/30\n",
      "Processing 100u (ID: 228246) - 5/30\n",
      "Processing 100v (ID: 228247) - 6/30\n",
      "Processing cdir (ID: 228022) - 7/30\n",
      "Processing uvb (ID: 57) - 8/30\n",
      "Processing ssr (ID: 176) - 9/30\n",
      "Processing ssrc (ID: 210) - 10/30\n",
      "Processing str (ID: 177) - 11/30\n",
      "Processing strc (ID: 211) - 12/30\n",
      "Processing ssrdc (ID: 228129) - 13/30\n",
      "Processing ssrd (ID: 169) - 14/30\n",
      "Processing strdc (ID: 228130) - 15/30\n",
      "Processing strd (ID: 175) - 16/30\n",
      "Processing tsr (ID: 178) - 17/30\n",
      "Processing tsrc (ID: 208) - 18/30\n",
      "Processing ttr (ID: 179) - 19/30\n",
      "Processing ttrc (ID: 209) - 20/30\n",
      "Processing fdir (ID: 228021) - 21/30\n",
      "Processing hcc (ID: 188) - 22/30\n",
      "Processing lcc (ID: 186) - 23/30\n",
      "Processing mcc (ID: 187) - 24/30\n",
      "Processing tcc (ID: 164) - 25/30\n",
      "Processing cvh (ID: 28) - 26/30\n",
      "Processing lai_hv (ID: 67) - 27/30\n",
      "Processing lai_lv (ID: 66) - 28/30\n",
      "Processing cvl (ID: 27) - 29/30\n",
      "Processing kx (ID: 260121) - 30/30\n",
      "\n",
      "✅ Enriched variable metadata:\n",
      "{'paramId': 167, 'shortName': '2t', 'fullName': '2 metre temperature', 'units': 'K', 'description': \"This parameter is the temperature of air at 2m above the surface of land, sea or in-land waters.\\n\\n2m temperature is calculated by interpolating between the lowest model level and the Earth's surface, taking account of the atmospheric conditions. See further information .\\n\\nThis parameter has units of kelvin (K). Temperature measured in kelvin can be converted to degrees Celsius (°C) by subtracting 273.15.\\n\\nPlease note that the encodings listed here for s2s & uerra (which includes encodings for carra/cerra) include entries for Mean 2 metre temperature. The specific encoding for Mean 2 metre temperature can be found in 228004.\", 'url': 'https://codes.ecmwf.int/grib/param-db/167/'}\n",
      "{'paramId': 228, 'shortName': 'tp', 'fullName': 'Total precipitation', 'units': 'm', 'description': 'This parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth\\'s surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. See further information. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth.\\n\\nThis parameter is the total amount of water accumulated over a particular time period which depends on the data extracted. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box.\\n\\nCare should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box. [NOTE: See 228228 for the equivalent parameter in \"kg m-2\"]', 'url': 'https://codes.ecmwf.int/grib/param-db/228/'}\n",
      "{'paramId': 165, 'shortName': '10u', 'fullName': '10 metre U wind component', 'units': 'm s-1', 'description': 'This parameter is the eastward component of the 10m wind. It is the horizontal speed of air moving towards the east, at a height of ten metres above the surface of the Earth, in metres per second.\\n\\nCare should be taken when comparing this parameter with observations, because wind observations vary on small space and time scales and are affected by the local terrain, vegetation and buildings that are represented only on average in the ECMWF Integrated Forecasting System.\\n\\nThis parameter can be combined with the V component of 10m wind to give the speed and direction of the horizontal 10m wind.', 'url': 'https://codes.ecmwf.int/grib/param-db/165/'}\n",
      "{'paramId': 166, 'shortName': '10v', 'fullName': '10 metre V wind component', 'units': 'm s-1', 'description': 'This parameter is the northward component of the 10m wind. It is the horizontal speed of air moving towards the north, at a height of ten metres above the surface of the Earth, in metres per second.\\n\\nCare should be taken when comparing this parameter with observations, because wind observations vary on small space and time scales and are affected by the local terrain, vegetation and buildings that are represented only on average in the ECMWF Integrated Forecasting System.\\n\\nThis parameter can be combined with the U component of 10m wind to give the speed and direction of the horizontal 10m wind.', 'url': 'https://codes.ecmwf.int/grib/param-db/166/'}\n",
      "{'paramId': 228246, 'shortName': '100u', 'fullName': '100 metre U wind component', 'units': 'm s-1', 'description': 'This parameter is the eastward component of the 100 m wind. It is the horizontal speed of air moving towards the east, at a height of 100 metres above the surface of the Earth, in metres per second.\\n\\nCare should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box and model time step.\\n\\nThis parameter can be combined with the northward component to give the speed and direction of the horizontal 100 m wind.', 'url': 'https://codes.ecmwf.int/grib/param-db/228246/'}\n",
      "{'paramId': 228247, 'shortName': '100v', 'fullName': '100 metre V wind component', 'units': 'm s-1', 'description': 'This parameter is the northward component of the 100 m wind. It is the horizontal speed of air moving towards the north, at a height of 100 metres above the surface of the Earth, in metres per second.\\n\\nCare should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box and model time step.\\n\\nThis parameter can be combined with the eastward component to give the speed and direction of the horizontal 100 m wind.', 'url': 'https://codes.ecmwf.int/grib/param-db/228247/'}\n",
      "{'paramId': 228022, 'shortName': 'cdir', 'fullName': 'Surface direct short-wave radiation, clear sky', 'units': 'J m-2', 'description': 'This parameter is the amount of direct radiation from the Sun (also known as solar or shortwave radiation) reaching the surface of the Earth, assuming clear-sky (cloudless) conditions. It is the amount of radiation passing through a horizontal plane, not a plane perpendicular to the direction of the Sun.\\n\\nSolar radiation at the surface can be direct or diffuse. Solar radiation can be scattered in all directions by particles in the atmosphere, some of which reaches the surface (diffuse solar radiation). Some solar radiation reaches the surface without being scattered (direct solar radiation). See further documentation.\\n\\nClear-sky radiation quantities are computed for exactly the same atmospheric conditions of temperature, humidity, ozone, trace gases and aerosol as the corresponding total-sky quantities (clouds included), but assuming that the clouds are not there.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/228022/'}\n",
      "{'paramId': 57, 'shortName': 'uvb', 'fullName': 'Surface downward UV radiation', 'units': 'J m-2', 'description': 'This parameter is the amount of ultraviolet (UV) radiation reaching the surface. It is the amount of radiation passing through a horizontal plane, not a plane perpendicular to the direction of the Sun.\\n\\nUV radiation is part of the electromagnetic spectrum emitted by the Sun that has wavelengths shorter than visible light. In the ECMWF Integrated Forecasting system it is defined as radiation with a wavelength of 0.20-0.44 µm (microns, 1 millionth of a metre). \\n\\nSmall amounts of UV are essential for living organisms, but overexposure may result in cell damage; in humans this includes acute and chronic health effects on the skin, eyes and immune system. UV radiation is absorbed by the ozone layer, but some reaches the surface. The depletion of the ozone layer is causing concern over an increase in the damaging effects of UV. \\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/57/'}\n",
      "{'paramId': 176, 'shortName': 'ssr', 'fullName': 'Surface net short-wave (solar) radiation', 'units': 'J m-2', 'description': \"This parameter is the amount of solar radiation (also known as shortwave radiation) that reaches a horizontal plane at the surface of the Earth (both direct and diffuse) minus the amount reflected by the Earth's surface (which is governed by the albedo).\\n\\nRadiation from the Sun (solar, or shortwave, radiation) is partly reflected back to space by clouds and particles in the atmosphere (aerosols) and some of it is absorbed. The remainder is incident on the Earth's surface, where some of it is reflected. See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds. The ECMWF convention for vertical fluxes is positive downwards.\", 'url': 'https://codes.ecmwf.int/grib/param-db/176/'}\n",
      "{'paramId': 210, 'shortName': 'ssrc', 'fullName': 'Surface net short-wave (solar) radiation, clear sky', 'units': 'J m-2', 'description': \"This parameter is the amount of solar (shortwave) radiation reaching the surface of the Earth (both direct and diffuse) minus the amount reflected by the Earth's surface (which is governed by the albedo), assuming clear-sky (cloudless) conditions. It is the amount of radiation passing through a horizontal plane, not a plane perpendicular to the direction of the Sun.\\n\\nClear-sky radiation quantities are computed for exactly the same atmospheric conditions of temperature, humidity, ozone, trace gases and aerosol as the corresponding total-sky quantities (clouds included), but assuming that the clouds are not there.\\n\\nRadiation from the Sun (solar, or shortwave, radiation) is partly reflected back to space by clouds and particles in the atmosphere (aerosols) and some of it is absorbed. The rest is incident on the Earth's surface, where some of it is reflected. The difference between downward and reflected solar radiation is the surface net solar radiation. See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.\", 'url': 'https://codes.ecmwf.int/grib/param-db/210/'}\n",
      "{'paramId': 177, 'shortName': 'str', 'fullName': 'Surface net long-wave (thermal) radiation', 'units': 'J m-2', 'description': 'Thermal radiation (also known as longwave or terrestrial radiation) refers to radiation emitted by the atmosphere, clouds and the surface of the Earth. This parameter is the difference between downward and upward thermal radiation at the surface of the Earth. It the amount passing through a horizontal plane.\\n\\nThe atmosphere and clouds emit thermal radiation in all directions, some of which reaches the surface as downward thermal radiation. The upward thermal radiation at the surface consists of thermal radiation emitted by the surface plus the fraction of downwards thermal radiation reflected upward by the surface. See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/177/'}\n",
      "{'paramId': 211, 'shortName': 'strc', 'fullName': 'Surface net long-wave (thermal) radiation, clear sky', 'units': 'J m-2', 'description': 'Thermal radiation (also known as longwave or terrestrial radiation) refers to radiation emitted by the atmosphere, clouds and the surface of the Earth. This parameter is the difference between downward and upward thermal radiation at the surface of the Earth, assuming clear-sky (cloudless) conditions. It is the amount of radiation passing through a horizontal plane. See further documentation.\\n\\nClear-sky radiation quantities are computed for exactly the same atmospheric conditions of temperature, humidity, ozone, trace gases and aerosol as the corresponding total-sky quantities (clouds included), but assuming that the clouds are not there.\\n\\nThe atmosphere and clouds emit thermal radiation in all directions, some of which reaches the surface as downward thermal radiation. The upward thermal radiation at the surface consists of thermal radiation emitted by the surface plus the fraction of downwards thermal radiation reflected upward by the surface. See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/211/'}\n",
      "{'paramId': 228129, 'shortName': 'ssrdc', 'fullName': 'Surface short-wave (solar) radiation downward clear-sky', 'units': 'J m-2', 'description': 'clear-sky downward shortwave radiation flux at surface computed from the model radiation scheme', 'url': 'https://codes.ecmwf.int/grib/param-db/228129/'}\n",
      "{'paramId': 169, 'shortName': 'ssrd', 'fullName': 'Surface short-wave (solar) radiation downwards', 'units': 'J m-2', 'description': \"This parameter is the amount of solar radiation (also known as shortwave radiation) that reaches a horizontal plane at the surface of the Earth. This parameter comprises both direct and diffuse solar radiation.\\n\\nRadiation from the Sun (solar, or shortwave, radiation) is partly reflected back to space by clouds and particles in the atmosphere (aerosols) and some of it is absorbed. The rest is incident on the Earth's surface (represented by this parameter). See further documentation.\\n\\nTo a reasonably good approximation, this parameter is the model equivalent of what would be measured by a pyranometer (an instrument used for measuring solar radiation) at the surface. However, care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds. The ECMWF convention for vertical fluxes is positive downwards.\", 'url': 'https://codes.ecmwf.int/grib/param-db/169/'}\n",
      "{'paramId': 228130, 'shortName': 'strdc', 'fullName': 'Surface long-wave (thermal) radiation downward clear-sky', 'units': 'J m-2', 'description': 'clear-sky downward longwave radiation flux at surface computed from the model radiation scheme', 'url': 'https://codes.ecmwf.int/grib/param-db/228130/'}\n",
      "{'paramId': 175, 'shortName': 'strd', 'fullName': 'Surface long-wave (thermal) radiation downwards', 'units': 'J m-2', 'description': 'This parameter is the amount of thermal (also known as longwave or terrestrial) radiation emitted by the atmosphere and clouds that reaches a horizontal plane at the surface of the Earth.\\n\\nThe surface of the Earth emits thermal radiation, some of which is absorbed by the atmosphere and clouds. The atmosphere and clouds likewise emit thermal radiation in all directions, some of which reaches the surface (represented by this parameter). See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds. The ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/175/'}\n",
      "{'paramId': 178, 'shortName': 'tsr', 'fullName': 'Top net short-wave (solar) radiation', 'units': 'J m-2', 'description': \"This parameter is the incoming solar radiation (also known as shortwave radiation) minus the outgoing solar radiation at the top of the atmosphere. It is the amount of radiation passing through a horizontal plane. The incoming solar radiation is the amount received from the Sun. The outgoing solar radiation is the amount reflected and scattered by the Earth's atmosphere and surface. See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.\", 'url': 'https://codes.ecmwf.int/grib/param-db/178/'}\n",
      "{'paramId': 208, 'shortName': 'tsrc', 'fullName': 'Top net short-wave (solar) radiation, clear sky', 'units': 'J m-2', 'description': \"This parameter is the incoming solar radiation (also known as shortwave radiation) minus the outgoing solar radiation at the top of the atmosphere, assuming clear-sky (cloudless) conditions. It is the amount of radiation passing through a horizontal plane. The incoming solar radiation is the amount received from the Sun. The outgoing solar radiation is the amount reflected and scattered by the Earth's atmosphere and surface, assuming clear-sky (cloudless) conditions. See further documentation.\\n\\nClear-sky radiation quantities are computed for exactly the same atmospheric conditions of temperature, humidity, ozone, trace gases and aerosol as the total-sky (clouds included) quantities, but assuming that the clouds are not there.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.\", 'url': 'https://codes.ecmwf.int/grib/param-db/208/'}\n",
      "{'paramId': 179, 'shortName': 'ttr', 'fullName': 'Top net long-wave (thermal) radiation', 'units': 'J m-2', 'description': 'The thermal (also known as terrestrial or longwave) radiation emitted to space at the top of the atmosphere is commonly known as the Outgoing Longwave Radiation (OLR). The top net thermal radiation (this parameter) is equal to the negative of OLR. See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.The ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/179/'}\n",
      "{'paramId': 209, 'shortName': 'ttrc', 'fullName': 'Top net long-wave (thermal) radiation, clear sky', 'units': 'J m-2', 'description': 'This parameter is the thermal (also known as terrestrial or longwave) radiation emitted to space at the top of the atmosphere, assuming clear-sky (cloudless) conditions. It is the amount passing through a horizontal plane. Note that the ECMWF convention for vertical fluxes is positive downwards, so a flux from the atmosphere to space will be negative. See further documentation.\\n\\nClear-sky radiation quantities are computed for exactly the same atmospheric conditions of temperature, humidity, ozone, trace gases and aerosol as total-sky quantities (clouds included), but assuming that the clouds are not there.\\n\\nThe thermal radiation emitted to space at the top of the atmosphere is commonly known as the Outgoing Longwave Radiation (OLR) (i.e., taking a flux from the atmosphere to space as positive). Note that OLR is typically shown in units of watts per square metre (W m-2).\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.', 'url': 'https://codes.ecmwf.int/grib/param-db/209/'}\n",
      "{'paramId': 228021, 'shortName': 'fdir', 'fullName': 'Surface direct short-wave (solar) radiation', 'units': 'J m-2', 'description': 'This parameter is the amount of direct solar radiation (also known as shortwave radiation) reaching the surface of the Earth. It is the amount of radiation passing through a horizontal plane, not a plane perpendicular to the direction of the Sun.\\n\\nSolar radiation at the surface can be direct or diffuse. Solar radiation can be scattered in all directions by particles in the atmosphere, some of which reaches the surface (diffuse solar radiation). Some solar radiation reaches the surface without being scattered (direct solar radiation). See further documentation.\\n\\nThis parameter is accumulated over a particular time period which depends on the data extracted. The units are joules per square metre (J m-2). To convert to watts per square metre (W m-2), the accumulated values should be divided by the accumulation period expressed in seconds.\\n\\nThe ECMWF convention for vertical fluxes is positive downwards.', 'url': 'https://codes.ecmwf.int/grib/param-db/228021/'}\n",
      "{'paramId': 188, 'shortName': 'hcc', 'fullName': 'High cloud cover', 'units': '(0 - 1)', 'description': 'The proportion of a grid box covered by cloud occurring in the high levels of the troposphere. High cloud is a single level field calculated from cloud occurring on model levels with a pressure less than 0.45 times the surface pressure. So, if the surface pressure is 1000 hPa (hectopascal), high cloud would be calculated using levels with a pressure of less than 450 hPa (approximately 6km and above ( assuming a `standard atmosphere`)).\\n\\nThe high cloud cover parameter is calculated from cloud for the appropriate model levels as described above. Assumptions are made about the degree of overlap/randomness between clouds in different model levels.\\n\\nCloud fractions vary from 0 to 1.\\n[NOTE: See 3075 for the equivalent parameter in \"%\"]', 'url': 'https://codes.ecmwf.int/grib/param-db/188/'}\n",
      "{'paramId': 186, 'shortName': 'lcc', 'fullName': 'Low cloud cover', 'units': '(0 - 1)', 'description': 'This parameter is the proportion of a grid box covered by cloud occurring in the lower levels of the troposphere. Low cloud is a single level field calculated from cloud occurring on model levels with a pressure greater than 0.8 times the surface pressure. So, if the surface pressure is 1000 hPa (hectopascal), low cloud would be calculated using levels with a pressure greater than 800 hPa (below approximately 2km (assuming a \\'standard atmosphere\\')).\\n\\nThe low cloud cover parameter is calculated from cloud cover for the appropriate model levels as described above. Assumptions are made about the degree of overlap/randomness between clouds in different model levels.\\n\\nCloud fractions vary from 0 to 1.\\n[NOTE: See 3073 for the equivalent parameter in \"%\"]', 'url': 'https://codes.ecmwf.int/grib/param-db/186/'}\n",
      "{'paramId': 187, 'shortName': 'mcc', 'fullName': 'Medium cloud cover', 'units': '(0 - 1)', 'description': 'This parameter is the proportion of a grid box covered by cloud occurring in the middle levels of the troposphere. Medium cloud is a single level field calculated from cloud occurring on model levels with a pressure between 0.45 and 0.8 times the surface pressure. So, if the surface pressure is 1000 hPa (hectopascal), medium cloud would be calculated using levels with a pressure of less than or equal to 800 hPa and greater than or equal to 450 hPa (between approximately 2km and 6km (assuming a \\'standard atmosphere\\')).\\n\\nThe medium cloud parameter is calculated from cloud cover for the appropriate model levels as described above. Assumptions are made about the degree of overlap/randomness between clouds in different model levels.\\n\\nCloud fractions vary from 0 to 1.\\n[NOTE: See 3074 for the equivalent parameter in \"%\"]', 'url': 'https://codes.ecmwf.int/grib/param-db/187/'}\n",
      "{'paramId': 164, 'shortName': 'tcc', 'fullName': 'Total cloud cover', 'units': '(0 - 1)', 'description': 'This parameter is the proportion of a grid box covered by cloud. Total cloud cover is a single level field calculated from the cloud occurring at different model levels through the atmosphere. Assumptions are made about the degree of overlap/randomness between clouds at different heights.\\n\\nCloud fractions vary from 0 to 1.\\n[NOTE: See 228164 for the equivalent parameter in \"%\"]', 'url': 'https://codes.ecmwf.int/grib/param-db/164/'}\n",
      "{'paramId': 28, 'shortName': 'cvh', 'fullName': 'High vegetation cover', 'units': '(0 - 1)', 'description': \"This parameter is the fraction of the grid box (0-1) that is covered with vegetation that is classified as 'high'.\\n\\nThis is one of the parameters in the model that describes land surface vegetation. 'High vegetation' consists of evergreen trees, deciduous trees, mixed forest/woodland, and interrupted forest.\", 'url': 'https://codes.ecmwf.int/grib/param-db/28/'}\n",
      "{'paramId': 67, 'shortName': 'lai_hv', 'fullName': 'Leaf area index, high vegetation', 'units': 'm2 m-2', 'description': \"This parameter is the surface area of one side of all the leaves found over an area of land for vegetation classified as 'high'. This parameter has a value of 0 over bare ground or where there are no leaves. It can be calculated daily from satellite data. It is important for forecasting, for example, how much rainwater will be intercepted by the vegetative canopy, rather than falling to the ground.\\n\\nThis is one of the parameters in the model that describes land surface vegetation. 'High vegetation' consists of evergreen trees, deciduous trees, mixed forest/woodland, and interrupted forest.\", 'url': 'https://codes.ecmwf.int/grib/param-db/67/'}\n",
      "{'paramId': 66, 'shortName': 'lai_lv', 'fullName': 'Leaf area index, low vegetation', 'units': 'm2 m-2', 'description': \"This parameter is the surface area of one side of all the leaves found over an area of land for vegetation classified as 'low'. This parameter has a value of 0 over bare ground or where there are no leaves. It can be calculated daily from satellite data. It is important for forecasting, for example, how much rainwater will be intercepted by the vegetative canopy, rather than falling to the ground.\\n\\nThis is one of the parameters in the model that describes land surface vegetation. 'Low vegetation' consists of crops and mixed farming, irrigated crops, short grass, tall grass, tundra, semidesert, bogs and marshes, evergreen shrubs, deciduous shrubs, and water and land mixtures.\", 'url': 'https://codes.ecmwf.int/grib/param-db/66/'}\n",
      "{'paramId': 27, 'shortName': 'cvl', 'fullName': 'Low vegetation cover', 'units': '(0 - 1)', 'description': \"This parameter is the fraction of the grid box (0-1) that is covered with vegetation that is classified as 'low'.\\n\\nThis is one of the parameters in the model that describes land surface vegetation. 'Low vegetation' consists of crops and mixed farming, irrigated crops, short grass, tall grass, tundra, semidesert, bogs and marshes, evergreen shrubs, deciduous shrubs, and water and land mixtures.\", 'url': 'https://codes.ecmwf.int/grib/param-db/27/'}\n",
      "{'paramId': 260121, 'shortName': 'kx', 'fullName': 'K index', 'units': 'K', 'description': 'This parameter is a measure of potential for a thunderstorm to develop calculated from the temperature and dew point temperature in the lower part of the atmosphere. The calculation uses the temperature at 850, 700 and 500 hPa and dewpoint temperature at 850 and 700 hPa. Higher values of K indicate a higher potential for the development of thunderstorms.\\n\\nThis parameter is related to the probability of occurrence of a thunderstorm:\\nParameter value Thunderstorm Probability\\n<20 K No thunderstorm.\\n20-25 K Isolated thunderstorms.\\n26-30 K Widely scattered thunderstorms.\\n31-35 K Scattered thunderstorms.\\n>35 K Numerous thunderstorms', 'url': 'https://codes.ecmwf.int/grib/param-db/260121/'}\n"
     ]
    }
   ],
   "source": [
    "enriched = enrich_variable_list_selenium(vars)\n",
    "\n",
    "print(\"\\n✅ Enriched variable metadata:\")\n",
    "for v in enriched:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a2a1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables_from_first_file(files_by_year: Dict[int, List[Path]]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Use the first available file to detect ERA5 variable names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files_by_year : dict[int, list[pathlib.Path]]\n",
    "        Mapping of year → list of file paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set[str]\n",
    "        Set of variable names found in the first file.\n",
    "    \"\"\"\n",
    "    first_file = get_first_file_for_dataset(files_by_year)\n",
    "    print(f\"Using first file for variable scan: {first_file}\")\n",
    "    return scan_variables_in_file(first_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cac7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_variable_list(vars_found: Set[str]) -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print the list of detected variable names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vars_found : set[str]\n",
    "        Variable names to print.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"\\nDetected the following variables in the first file:\\n\")\n",
    "    for v in sorted(vars_found):\n",
    "        print(f\"  • {v}\")\n",
    "    print(f\"\\nTotal variables: {len(vars_found)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2020fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_variables_across_files(\n",
    "        files_by_year: Dict[int, List[Path]],\n",
    "        reference_vars: Set[str]\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Optionally verify that all files share the same set of variable names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files_by_year : dict[int, list[pathlib.Path]]\n",
    "        Mapping of year → list of file paths.\n",
    "    reference_vars : set[str]\n",
    "        Reference set of variables from the first file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Prints warnings if any file is found with a different variable set.\n",
    "    \"\"\"\n",
    "    print(\"\\nVerifying that all files share the same variable structure...\\n\")\n",
    "    mismatches = []\n",
    "\n",
    "    for year, paths in files_by_year.items():\n",
    "        for path in paths:\n",
    "            vars_this = scan_variables_in_file(path)\n",
    "            if vars_this and vars_this != reference_vars:\n",
    "                diff_ref = reference_vars - vars_this\n",
    "                diff_this = vars_this - reference_vars\n",
    "                mismatches.append((path, diff_ref, diff_this))\n",
    "\n",
    "    if not mismatches:\n",
    "        print(\"✅ All checked files share the same variable set.\\n\")\n",
    "    else:\n",
    "        print(\"⚠ Detected files with differing variable sets:\\n\")\n",
    "        for path, missing_from_file, extra_in_file in mismatches:\n",
    "            print(f\"File: {path}\")\n",
    "            if missing_from_file:\n",
    "                print(\"  Missing (present in reference but not in this file):\")\n",
    "                for v in sorted(missing_from_file):\n",
    "                    print(f\"    - {v}\")\n",
    "            if extra_in_file:\n",
    "                print(\"  Extra (present in this file but not in reference):\")\n",
    "                for v in sorted(extra_in_file):\n",
    "                    print(f\"    - {v}\")\n",
    "            print()\n",
    "\n",
    "        cont = prompt_yes_no(\"Differences detected. Continue processing anyway?\",\n",
    "                             default=\"n\")\n",
    "        if not cont:\n",
    "            raise RuntimeError(\"Aborting due to variable set mismatches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bf32167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_period_ids_for_year(\n",
    "        year: int,\n",
    "        agg_level: str\n",
    "        ) -> List[Tuple[int, Optional[str]]]:\n",
    "    \"\"\"\n",
    "    Generate (month, period_id) pairs for the given year and aggregation level.\n",
    "\n",
    "    This is used to decide how to slice the data and how to name files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Year of interest.\n",
    "    agg_level : {'annual', 'half-year', 'quarterly', 'monthly'}\n",
    "        Aggregation level.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of (int, str or None)\n",
    "        Each entry is a tuple (month, period_id), where:\n",
    "        - For 'annual':    one entry with (None, None)\n",
    "        - For 'half-year': months are grouped into 'A1' (Jan–Jun), 'A2' (Jul–Dec)\n",
    "        - For 'quarterly': months grouped into 'Q1'..'Q4'\n",
    "        - For 'monthly':   one period per month with 'M01'..'M12'\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For processing convenience we primarily use this to derive the period\n",
    "    identifier (like 'A1', 'Q3', 'M05') and then filter rows by month ranges.\n",
    "    \"\"\"\n",
    "    if agg_level == \"annual\":\n",
    "        return [(None, None)]  # single period for whole year\n",
    "\n",
    "    if agg_level == \"monthly\":\n",
    "        return [(m, f\"M{m:02d}\") for m in range(1, 13)]\n",
    "\n",
    "    if agg_level == \"half-year\":\n",
    "        # A1: Jan–Jun, A2: Jul–Dec\n",
    "        return [\n",
    "            (1, \"A1\"),  # we use the month just as a handle here\n",
    "            (7, \"A2\"),\n",
    "        ]\n",
    "\n",
    "    if agg_level == \"quarterly\":\n",
    "        return [\n",
    "            (1, \"Q1\"),   # Jan–Mar\n",
    "            (4, \"Q2\"),   # Apr–Jun\n",
    "            (7, \"Q3\"),   # Jul–Sep\n",
    "            (10, \"Q4\"),  # Oct–Dec\n",
    "        ]\n",
    "\n",
    "    raise ValueError(f\"Unknown aggregation level: {agg_level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3028fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_filename(\n",
    "        prefix: str,\n",
    "        year: int,\n",
    "        agg_level: str,\n",
    "        period_id: Optional[str]\n",
    "        ) -> str:\n",
    "    \"\"\"\n",
    "    Construct an output filename for a given prefix/year/aggregation/period.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Annual:\n",
    "        prefix_annual_2020.parquet\n",
    "\n",
    "    Half-year:\n",
    "        prefix_halfyear_2020_A1.parquet\n",
    "\n",
    "    Quarterly:\n",
    "        prefix_quarterly_2020_Q3.parquet\n",
    "\n",
    "    Monthly:\n",
    "        prefix_monthly_2020_M07.parquet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prefix : str\n",
    "        Dataset prefix (no extension and no spaces/parentheses).\n",
    "    year : int\n",
    "        Year of the data.\n",
    "    agg_level : {'annual', 'half-year', 'quarterly', 'monthly'}\n",
    "        Aggregation/split level.\n",
    "    period_id : str or None\n",
    "        Period identifier (e.g., 'A1', 'Q3', 'M07'); None for annual.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Filename (without directory), always ending in '.parquet'.\n",
    "    \"\"\"\n",
    "    if agg_level == \"annual\":\n",
    "        return f\"{prefix}_annual_{year}.parquet\"\n",
    "\n",
    "    if agg_level == \"half-year\":\n",
    "        if period_id is None:\n",
    "            raise ValueError(\"period_id must not be None for half-year.\")\n",
    "        return f\"{prefix}_halfyear_{year}_{period_id}.parquet\"\n",
    "\n",
    "    if agg_level == \"quarterly\":\n",
    "        if period_id is None:\n",
    "            raise ValueError(\"period_id must not be None for quarterly.\")\n",
    "        return f\"{prefix}_quarterly_{year}_{period_id}.parquet\"\n",
    "\n",
    "    if agg_level == \"monthly\":\n",
    "        if period_id is None:\n",
    "            raise ValueError(\"period_id must not be None for monthly.\")\n",
    "        return f\"{prefix}_monthly_{year}_{period_id}.parquet\"\n",
    "\n",
    "    raise ValueError(f\"Unknown aggregation level: {agg_level}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb3691",
   "metadata": {},
   "source": [
    "##### Functions - Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb3ff68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_yes_no(\n",
    "        message: str,\n",
    "        default: str = \"y\"\n",
    "        ) -> bool:\n",
    "    \"\"\"\n",
    "    Generic yes/no prompt.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    message : str\n",
    "        Question to display to the user.\n",
    "    default : {'y', 'n'}, optional\n",
    "        Default answer if the user just presses ENTER.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if user answers yes, False otherwise.\n",
    "    \"\"\"\n",
    "    default = default.lower()\n",
    "    assert default in (\"y\", \"n\")\n",
    "\n",
    "    suffix = \"[Y/n]\" if default == \"y\" else \"[y/N]\"\n",
    "\n",
    "    while True:\n",
    "        ans = input(f\"{message} {suffix}: \").strip().lower()\n",
    "        if ans == \"\" and default in (\"y\", \"n\"):\n",
    "            return default == \"y\"\n",
    "        if ans in (\"y\", \"yes\"):\n",
    "            return True\n",
    "        if ans in (\"n\", \"no\"):\n",
    "            return False\n",
    "        print(\"❌ Please answer 'y' or 'n'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c0db689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_for_columns_to_drop(default_drop: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Prompt the user to confirm or override the list of columns to drop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    default_drop : list of str or None, optional\n",
    "        Default list of columns to drop. If None or empty, no columns are\n",
    "        dropped unless the user specifies them manually.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Final list of columns to drop.\n",
    "    \"\"\"\n",
    "    default_drop = default_drop or []\n",
    "\n",
    "    if default_drop:\n",
    "        print(\"\\nThe following columns are currently marked for dropping:\")\n",
    "        for col in default_drop:\n",
    "            print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(\"\\nNo default columns are marked for dropping.\")\n",
    "\n",
    "    user_input = input(\n",
    "        \"\\nPress ENTER to accept this selection, or enter a comma-separated \"\n",
    "        \"list of column names to drop instead: \"\n",
    "    ).strip()\n",
    "\n",
    "    if user_input == \"\":\n",
    "        final_drop = default_drop\n",
    "    else:\n",
    "        final_drop = [c.strip() for c in user_input.split(\",\") if c.strip()]\n",
    "\n",
    "    print(\"\\nFinal list of columns to drop:\")\n",
    "    if final_drop:\n",
    "        for col in final_drop:\n",
    "            print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(\"  (None)\")\n",
    "    print()\n",
    "\n",
    "    return final_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a778fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_for_aggregation_level() -> str:\n",
    "    \"\"\"\n",
    "    Prompt the user to choose an aggregation/split level.\n",
    "\n",
    "    Options\n",
    "    -------\n",
    "    - 'annual'\n",
    "    - 'half-year'\n",
    "    - 'quarterly'\n",
    "    - 'monthly'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        One of {'annual', 'half-year', 'quarterly', 'monthly'}.\n",
    "    \"\"\"\n",
    "    valid = {\"annual\", \"half-year\", \"quarterly\", \"monthly\"}\n",
    "\n",
    "    print(\"\\nPlease select the aggregation/split level for output files:\")\n",
    "    print(\"  - annual\")\n",
    "    print(\"  - half-year\")\n",
    "    print(\"  - quarterly\")\n",
    "    print(\"  - monthly\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nEnter aggregation level: \").strip().lower()\n",
    "        if choice in valid:\n",
    "            print(f\"\\nYou selected: {choice}\\n\")\n",
    "            return choice\n",
    "        print(\"❌ Invalid choice. Please enter one of: annual, half-year, quarterly, monthly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f03e6",
   "metadata": {},
   "source": [
    "##### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b641817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.print_variable_list(vars_found: 'Set[str]') -> 'None'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_first_file_for_dataset(mapping[selected_key])\n",
    "scan_variables_in_file(path)\n",
    "get_variables_from_first_file(mapping[selected_key])\n",
    "print_variable_list(get_variables_from_first_file(mapping[selected_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e8d12",
   "metadata": {},
   "source": [
    "#### Part 3 - Data Processing - Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e4fe5",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ab7725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grib_file_to_polars(path: Path) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single GRIB file using xarray and convert it to a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : pathlib.Path\n",
    "        Path to the GRIB file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    polars.DataFrame\n",
    "        DataFrame containing all variables and coordinates.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses xarray with cfgrib engine.\n",
    "    - Assumes presence of a 'time' coordinate or a 'valid_time' coordinate.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"indexpath\": \"\"}\n",
    "    )\n",
    "\n",
    "    # Flatten to pandas then to polars\n",
    "    pdf = ds.to_dataframe().reset_index()\n",
    "    ds.close()\n",
    "\n",
    "    pl_df = pl.from_pandas(pdf)\n",
    "\n",
    "    # Normalise time column\n",
    "    if \"time\" in pl_df.columns:\n",
    "        time_col = \"time\"\n",
    "    elif \"valid_time\" in pl_df.columns:\n",
    "        time_col = \"valid_time\"\n",
    "    else:\n",
    "        raise ValueError(f\"No 'time' or 'valid_time' column found in {path}.\")\n",
    "\n",
    "    pl_df = pl_df.with_columns(\n",
    "        pl.col(time_col).alias(\"timestamp\")\n",
    "    ).drop(time_col)\n",
    "\n",
    "    # Ensure timestamp is a proper datetime type\n",
    "    pl_df = pl_df.with_columns(\n",
    "        pl.col(\"timestamp\").cast(pl.Datetime(time_unit=\"us\"), strict=False)\n",
    "    )\n",
    "\n",
    "    return pl_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74fe2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_year_files(\n",
    "        files_by_year: Dict[int, List[Path]],\n",
    "        year: int\n",
    "        ) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Load and vertically concatenate all files for a given year into a LazyFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files_by_year : dict[int, list[pathlib.Path]]\n",
    "        Mapping of year → list of file paths.\n",
    "    year : int\n",
    "        Year to combine.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    polars.LazyFrame\n",
    "        Combined LazyFrame containing all rows for that year.\n",
    "    \"\"\"\n",
    "    paths = files_by_year.get(year, [])\n",
    "    if not paths:\n",
    "        raise ValueError(f\"No files for year {year}.\")\n",
    "\n",
    "    dfs: List[pl.DataFrame] = []\n",
    "    for p in sorted(paths):\n",
    "        print(f\"   Loading {p} ...\")\n",
    "        df = load_grib_file_to_polars(p)\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined = pl.concat(dfs, how=\"vertical\")\n",
    "    return combined.lazy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab2af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_and_save_year(\n",
    "        lf_year: pl.LazyFrame,\n",
    "        prefix: str,\n",
    "        year: int,\n",
    "        agg_level: str,\n",
    "        drop_cols: List[str],\n",
    "        output_dir: Path\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Partition a year's LazyFrame by the chosen aggregation level and save\n",
    "    each partition to a separate parquet file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lf_year : polars.LazyFrame\n",
    "        LazyFrame containing all data for a given year.\n",
    "    prefix : str\n",
    "        Dataset prefix (no extension, no parentheses).\n",
    "    year : int\n",
    "        Year of the data.\n",
    "    agg_level : {'annual', 'half-year', 'quarterly', 'monthly'}\n",
    "        Aggregation/split level.\n",
    "    drop_cols : list[str]\n",
    "        Columns to drop before writing.\n",
    "    output_dir : pathlib.Path\n",
    "        Output directory for parquet files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Add year and month derived from timestamp\n",
    "    lf = lf_year.with_columns([\n",
    "        pl.col(\"timestamp\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"timestamp\").dt.month().alias(\"month\"),\n",
    "    ])\n",
    "\n",
    "    # Drop columns if requested\n",
    "    if drop_cols:\n",
    "        # Filter drop_cols to those that actually exist\n",
    "        cols_in_schema = set(lf.collect_schema().names())\n",
    "        actual_drop = [c for c in drop_cols if c in cols_in_schema]\n",
    "        if actual_drop:\n",
    "            lf = lf.drop(actual_drop)\n",
    "        else:\n",
    "            print(\"Warning: none of the requested drop columns exist in this schema.\")\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    period_specs = make_period_ids_for_year(year, agg_level)\n",
    "\n",
    "    if agg_level == \"annual\":\n",
    "        # One file for the whole year\n",
    "        fname = make_output_filename(prefix, year, agg_level, period_id=None)\n",
    "        out_path = output_dir / fname\n",
    "        print(f\"   Writing annual file: {out_path}\")\n",
    "        lf.filter(pl.col(\"year\") == year).sink_parquet(out_path)\n",
    "        return\n",
    "\n",
    "    for month_anchor, period_id in period_specs:\n",
    "        if agg_level == \"monthly\":\n",
    "            # Filter rows for this month\n",
    "            m = month_anchor\n",
    "            fname = make_output_filename(prefix, year, agg_level, period_id)\n",
    "            out_path = output_dir / fname\n",
    "            print(f\"   Writing monthly file: {out_path}\")\n",
    "\n",
    "            lf.filter((pl.col(\"year\") == year) & (pl.col(\"month\") == m)) \\\n",
    "              .sink_parquet(out_path)\n",
    "\n",
    "        elif agg_level == \"half-year\":\n",
    "            # A1: Jan–Jun, A2: Jul–Dec\n",
    "            if period_id == \"A1\":\n",
    "                cond = (pl.col(\"month\") >= 1) & (pl.col(\"month\") <= 6)\n",
    "            elif period_id == \"A2\":\n",
    "                cond = (pl.col(\"month\") >= 7) & (pl.col(\"month\") <= 12)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown half-year period_id: {period_id}\")\n",
    "\n",
    "            fname = make_output_filename(prefix, year, agg_level, period_id)\n",
    "            out_path = output_dir / fname\n",
    "            print(f\"   Writing half-year file: {out_path}\")\n",
    "            lf.filter((pl.col(\"year\") == year) & cond).sink_parquet(out_path)\n",
    "\n",
    "        elif agg_level == \"quarterly\":\n",
    "            # Q1: Jan–Mar, Q2: Apr–Jun, Q3: Jul–Sep, Q4: Oct–Dec\n",
    "            if period_id == \"Q1\":\n",
    "                cond = (pl.col(\"month\") >= 1) & (pl.col(\"month\") <= 3)\n",
    "            elif period_id == \"Q2\":\n",
    "                cond = (pl.col(\"month\") >= 4) & (pl.col(\"month\") <= 6)\n",
    "            elif period_id == \"Q3\":\n",
    "                cond = (pl.col(\"month\") >= 7) & (pl.col(\"month\") <= 9)\n",
    "            elif period_id == \"Q4\":\n",
    "                cond = (pl.col(\"month\") >= 10) & (pl.col(\"month\") <= 12)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown quarterly period_id: {period_id}\")\n",
    "\n",
    "            fname = make_output_filename(prefix, year, agg_level, period_id)\n",
    "            out_path = output_dir / fname\n",
    "            print(f\"   Writing quarterly file: {out_path}\")\n",
    "            lf.filter((pl.col(\"year\") == year) & cond).sink_parquet(out_path)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation level: {agg_level}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d0c8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(\n",
    "        dataset_key: str,\n",
    "        files_by_year: Dict[int, List[Path]],\n",
    "        year: int,\n",
    "        agg_level: str,\n",
    "        drop_cols: List[str],\n",
    "        output_dir: Path\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Process all files for a single year: load, combine, partition, and save.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_key : str\n",
    "        Dataset key, e.g. \"era5-world_N37... (grib)\".\n",
    "    files_by_year : dict[int, list[pathlib.Path]]\n",
    "        Mapping of year → list of file paths.\n",
    "    year : int\n",
    "        Year to process.\n",
    "    agg_level : str\n",
    "        Aggregation/split level: 'annual', 'half-year', 'quarterly', 'monthly'.\n",
    "    drop_cols : list[str]\n",
    "        Columns to drop.\n",
    "    output_dir : pathlib.Path\n",
    "        Output directory for parquet files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    prefix = dataset_key.split(\" (\")[0]  # strip \" (ext)\"\n",
    "    print(f\"\\n=== Processing year {year} for dataset '{dataset_key}' ===\")\n",
    "    lf_year = combine_year_files(files_by_year, year)\n",
    "    partition_and_save_year(lf_year, prefix, year, agg_level, drop_cols, output_dir)\n",
    "    print(f\"=== Finished year {year} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1b26f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(\n",
    "        dataset_key: str,\n",
    "        files_by_year: Dict[int, List[Path]],\n",
    "        agg_level: str,\n",
    "        drop_cols: List[str],\n",
    "        output_dir: Path,\n",
    "        max_workers: int = 4\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Process an entire dataset across all available years with optional\n",
    "    parallelisation (one worker per year).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_key : str\n",
    "        Dataset key, e.g. \"era5-world_N37... (grib)\".\n",
    "    files_by_year : dict[int, list[pathlib.Path]]\n",
    "        Mapping of year → list of file paths for the selected dataset.\n",
    "    agg_level : str\n",
    "        Aggregation/split level: 'annual', 'half-year', 'quarterly', 'monthly'.\n",
    "    drop_cols : list[str]\n",
    "        Columns to drop.\n",
    "    output_dir : pathlib.Path\n",
    "        Output directory for parquet files.\n",
    "    max_workers : int, optional\n",
    "        Maximum number of worker threads to use for parallel processing of years.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    years = sorted(files_by_year.keys())\n",
    "    if not years:\n",
    "        print(\"No years found to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nStarting processing for dataset '{dataset_key}' with aggregation: {agg_level}\")\n",
    "    print(f\"Years to process: {years}\")\n",
    "    print(f\"Output directory: {output_dir.resolve()}\\n\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_year,\n",
    "                dataset_key,\n",
    "                files_by_year,\n",
    "                year,\n",
    "                agg_level,\n",
    "                drop_cols,\n",
    "                output_dir,\n",
    "            )\n",
    "            for year in years\n",
    "        ]\n",
    "        for fut in concurrent.futures.as_completed(futures):\n",
    "            exc = fut.exception()\n",
    "            if exc is not None:\n",
    "                print(f\"Error in worker: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9f993",
   "metadata": {},
   "source": [
    "#### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc326f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_interactive():\n",
    "    \"\"\"\n",
    "    Full interactive workflow:\n",
    "\n",
    "    1. Prompt for directory.\n",
    "    2. Scan directory, show dataset summary.\n",
    "    3. Prompt for dataset key.\n",
    "    4. Scan variables from first file.\n",
    "    5. Optionally verify structure across all files.\n",
    "    6. Prompt for columns to drop (default: none).\n",
    "    7. Prompt for aggregation level.\n",
    "    8. Process dataset into time-split parquet files.\n",
    "    \"\"\"\n",
    "    # 1. Directory selection\n",
    "    raw_dir = prompt_user_for_directory(default_dir=\"data/raw\")\n",
    "\n",
    "    # 2. Scan & summary\n",
    "    mapping = scan_and_optionally_print(raw_dir, print_summary=True)\n",
    "\n",
    "    # 3. Dataset key selection\n",
    "    selected_key = prompt_user_for_dataset_key(mapping)\n",
    "    files_by_year = mapping[selected_key]\n",
    "\n",
    "    # 4. Variable scan from first file\n",
    "    vars_first = get_variables_from_first_file(files_by_year)\n",
    "    print_variable_list(vars_first)\n",
    "\n",
    "    # 5. Optional structure verification\n",
    "    if prompt_yes_no(\"Would you like to verify that all files share the same structure?\",\n",
    "                     default=\"y\"):\n",
    "        verify_variables_across_files(files_by_year, vars_first)\n",
    "    else:\n",
    "        print(\"Skipping structure verification.\\n\")\n",
    "\n",
    "    # 6. Column drop prompt (no defaults for now)\n",
    "    drop_cols = prompt_for_columns_to_drop(default_drop=[])\n",
    "\n",
    "    # 7. Aggregation level prompt\n",
    "    agg_level = prompt_for_aggregation_level()\n",
    "\n",
    "    # 8. Process dataset into parquet\n",
    "    output_dir = Path(\"data/processed\")\n",
    "    # Simple heuristic: one worker per year, up to 8\n",
    "    max_workers = min(len(files_by_year), 8) or 1\n",
    "\n",
    "    process_dataset(\n",
    "        dataset_key=selected_key,\n",
    "        files_by_year=files_by_year,\n",
    "        agg_level=agg_level,\n",
    "        drop_cols=drop_cols,\n",
    "        output_dir=output_dir,\n",
    "        max_workers=max_workers,\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ All done.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ad504",
   "metadata": {},
   "source": [
    "#### Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6687a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the relative directory you want to scan.\n",
      "Press ENTER to use the default: [data/raw]\n",
      "Example inputs: data/raw, data/new_data, datasets/era5, etc.\n",
      "\n",
      "[ERROR] Directory 'data/raw' does not exist. Please try again.\n",
      "\n",
      "\n",
      "Using directory: /Users/Daniel/Desktop/data/raw\n",
      "\n",
      "\n",
      "Detected the following datasets in [../data/raw]:\n",
      "\n",
      "----------------------------------------\n",
      "era5-world_N37W68S6E98_d514a3a3c256 (grib)\n",
      "----------------------------------------\n",
      "\tYears found: 2018, 2019, 2020, 2023, 2024, 2025\n",
      "\t\t2018: 01, 02\n",
      "\t\t2019: 08, 12\n",
      "\t\t2020: 01\n",
      "\t\t2023: 02, 12\n",
      "\t\t2024: 01, 03, 04\n",
      "\t\t2025: 04, 06\n",
      "\n",
      "----------------------------------------\n",
      "era5-world_N37W68S6E98_d514a3a3c256 (grib.5b7b6.idx)\n",
      "----------------------------------------\n",
      "\tYears found: 2018\n",
      "\t\t2018: 01\n",
      "\n",
      "\n",
      "Available dataset prefixes:\n",
      "  1. era5-world_N37W68S6E98_d514a3a3c256 (grib)\n",
      "  2. era5-world_N37W68S6E98_d514a3a3c256 (grib.5b7b6.idx)\n",
      "\n",
      "You selected: era5-world_N37W68S6E98_d514a3a3c256 (grib)\n",
      "\n",
      "Using first file for variable scan: ../data/raw/era5-world_N37W68S6E98_d514a3a3c256_2018_01.grib\n",
      "\n",
      "Detected the following variables in the first file:\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain_interactive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mmain_interactive\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 4. Variable scan from first file\u001b[39;00m\n\u001b[32m     25\u001b[39m vars_first = get_variables_from_first_file(files_by_year)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mprint_variable_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvars_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 5. Optional structure verification\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompt_yes_no(\u001b[33m\"\u001b[39m\u001b[33mWould you like to verify that all files share the same structure?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m                  default=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mprint_variable_list\u001b[39m\u001b[34m(vars_found)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mPretty-print the list of detected variable names.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDetected the following variables in the first file:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvars_found\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  • \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vars_found)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "main_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18983aeb",
   "metadata": {},
   "source": [
    "#### Parsing ERA5 Filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4977724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "def scan_grib_variables_xarray(files: dict[int, list[Path]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Scan GRIB files using xarray+cfgrib to extract unique variable shortNames.\n",
    "    Does not load any full data arrays.\n",
    "    \"\"\"\n",
    "    shortnames = set()\n",
    "\n",
    "    for year, paths in files.items():\n",
    "        for path in paths:\n",
    "            try:\n",
    "                # Open the GRIB file *without* selecting variables\n",
    "                # Will fail intentionally, giving us available citations\n",
    "                xr.open_dataset(\n",
    "                    path,\n",
    "                    engine=\"cfgrib\",\n",
    "                    backend_kwargs={\"indexpath\": \"\"}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # cfgrib includes the available 'shortName' values in the exception message\n",
    "                msg = str(e)\n",
    "                if \"shortName\" in msg and \"values\" in msg:\n",
    "                    # Example:\n",
    "                    # \"Found multiple values for key 'shortName': ['t2m', 'u10', 'v10']\"\n",
    "                    import re\n",
    "                    found = re.findall(r\"\\['([^]]+)'\\]\", msg)\n",
    "                    if found:\n",
    "                        # Split comma-separated values inside the brackets\n",
    "                        vars_split = found[0].replace(\"'\", \"\").split(\",\")\n",
    "                        vars_clean = [v.strip() for v in vars_split]\n",
    "                        shortnames.update(vars_clean)\n",
    "\n",
    "    return shortnames\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
